---
title: "Using the BCDA toolset"
author: "Mikhail Popov"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
options(digits = 4)
library(ggplot2)
library(ggthemes)
theme_set(ggthemes::theme_tufte(base_family = "Gill Sans"))
library(magrittr)
```

The following table is from a report on the relationship between aspirin use and heart attacks by the Physicians' Health Study Research Group at Harvard Medical School. It was a 5-year randomized study of whether regular aspirin take reduces mortality from cardiovascular disease. Of the 11,034 physicians taking a placebo, 18 and 17 suffered fatal and non-fatal heart attacks, respectively. Of the 11,037 taking aspirin, 5 and 99 suffered fatal and non-fatal heart attacks, respectively.

```{r data, echo = FALSE}
aspirin <- matrix(c(18 + 171, 10845, 5 + 99, 10933), nrow = 2, byrow = TRUE)
rownames(aspirin) <- c("Placebo", "Aspirin")
colnames(aspirin) <- c("Myocardial Infraction", "No Attack")
aspirin <- aspirin[2:1, 2:1]
knitr::kable(addmargins(aspirin))
```

## Frequentist Approach

Using the frequentist approach, the individual cell probabilities $\pi_{ij}$ are estimated via $p_{ij} = n_{ij}/n_{++}$:

```{r probabilities, echo = FALSE}
knitr::kable(addmargins(prop.table(aspirin)), digits = 3)
```

We can perform a null hypothesis significance test of association using chi-square test or Fisher's exact test, both of which yield p-values > 0.001.

```{r chisq_indepen}
stats::chisq.test(aspirin) # usually done for large sample tables
```

```{r fisher_exact}
stats::fisher.test(aspirin) # usually done for small sample tables
```

The two major problems with the frequentist approach are:

1. The rarity of the disease.
2. The large sample size yields a tiny p-value for a small effect, so we call it *highly statistically significant*.

As Andrew Gelman [said](http://andrewgelman.com/2009/06/18/the_sample_size/), "In general: small n, unlikely to get small p-values. Large n, likely to find something. Huge n, almost certain to find lots of small p-values."

Alternatively, we can abandon null hypothesis signifiance test (NHST) approach that yields a p-value and instead employ Bayesian methods (which have their own quirks).

## Bayesian Approach

In Bayesian statistics, we are concerned with the posterior distribution of the parameter(s) of interest $\theta$ given the data. Using Bayes Theorem, we can express the posterior distribution $p(\theta~|~\text{data})$ as proportional to the product of the likelihood $p(\text{data}~|~\theta)$ and the prior $p(\theta)$: $$p(\theta~|~\text{data}) \propto p(\text{data}~|~\theta)~p(\theta).$$

In essence, while the classical (frequentist) approach regards $\theta$ as a fixed entity whose value is to be estimated, Bayesian perspective views $\theta$ as a random variable whose posterior probability distribution is to be estimated.

```{r packages}
library(BCDA)
```

We can use the `est_multinom` function to estimate the multinomial cell probabilities ($\pi_{11}, \pi_{12}, \pi_{21}, \pi_{22}$) using methods developed by Fienberg and Holland (1973). When we don't provide hyperparameters $\gamma_{ij}$, they are calculated using a simple model ($\gamma_{ij} = p_{i+}p_{+j}$).

```{r bayes_probabilities_example, eval = FALSE}
est_multinom(aspirin)
```

```{r bayes_probabilities, echo = FALSE}
knitr::kable(addmargins(est_multinom(aspirin)), digits = 3)
```

We can also employ the knowledge that we do have about the prevalence of heart attacks in U.S. via the [American Heart Association](https://www.heart.org/idc/groups/heart-public/@wcm/@sop/@smd/documents/downloadable/ucm_449846.pdf).

```{r prevalence, echo = FALSE}
prevalence <- data.frame(Incidence = c(0.8, 2.2, 0.2, 1.0,
                                       2.0, 3.6, 1.0, 2.3,
                                       3.8, 5.7, 2.0, 3.7,
                                       6.6, 8.1, 3.6, 7.2,
                                       9.1, 12.9, 7.8, 10.2),
                         Age = c(rep('35-44', 4), rep('45-54', 4), rep('55-64', 4), rep('65-74', 4), rep('75-84', 4)),
                         Sex = c('Men', 'Women')[rep(c(1,1,2,2), 5)],
                         Race = c('White', 'Black')[rep(rep(c(1,2), 2), 5)])
```

```{r prevalence_visualization, echo = FALSE, fig.width = 7, fig.height = 4}
suppressWarnings(library(ggplot2))
prevalence <- transform(prevalence, Group = paste(Race, Sex))
ggplot(data = prevalence,
       aes(y = Incidence, x = Age, fill = Group)) +
  geom_bar(stat = "identity", position = "dodge") +
  scale_y_continuous(name = "Prevalence per 1,000 persons") +
  ggtitle("Incidence of myocardial infraction\nSource: American Heart Association")
```

The prevalence (averaged across age, sex, and race) is 0.469%, which we can use as very naive prior information in our model.

```{r priors, echo = FALSE}
prior <- matrix(c(1-mean(prevalence$Incidence/1000), mean(prevalence$Incidence/1000)), ncol = 2, byrow = TRUE)
colnames(prior) <- c('No Attack', 'Avg Prevalence of M.I.')
knitr::kable(prior, digits = 3)
prior <- prior[c(1, 1), ]/2
rownames(prior) <- c('Aspirin', 'Placebo')
knitr::kable(prior, digits = 3)
```

```{r bayes_probabilities_prior_example, eval = FALSE}
est_multinom(aspirin, prior = prior)
```

```{r bayes_probabilities_prior, echo = FALSE}
knitr::kable(addmargins(est_multinom(aspirin, prior = prior)), digits = 3)
```

### Strength and Direction of Association

We assume an independent Binomial model with independent Beta priors on the probability parameter. For each group $i = 1, 2$ (aspirin and placebo): $$X_i \sim \text{Binom}(N_i, \pi_i),~\text{and}\\ \pi_i \sim \text{Beta}(\alpha_i, \beta_i).$$

However, we still need to provide the $\alpha$ and $\beta$ hyperparameters. We can set them to $\alpha=\beta=1$ which corresponds to a non-informative prior on $\pi$, or we can specify them such that the shape of the Beta prior better reflects our knowledge about the probability of (not) having a heart attack while being non-informative with respect to the groups. After all, we **don't** know which group has a better outcome, but if we did, we could specify the parameters to reflect that knowledge.

```{r beta_binom_stan_fit_example, message = FALSE}
set.seed(0)
fit <- beta_binom(aspirin)
```

We can also provide success counts and totals: `fit <- beta_binom(x = aspirin[, 1], n = aspirin[, 1] + aspirin[, 2])`

```{r beta_binom_stan_fit_example_plot, message = FALSE, fig.width = 7, fig.height = 4}
plot(fit, interval_type = "HPD") # requires coda package to be installed
# plot(fit) will use credible intervals computed using the quantile method
```

```{r beta_binom_stan_fit_example_summary_hpd}
summary(fit, interval_type = "HPD", digits = 2) # requires coda package to be installed
# summary(fit) will give credible intervals using the quantile method
```

```{r beta_binom_stan_fit_example_present_hpd}
present_bbfit(fit, interval_type = "HPD", digits = 2) # requires coda package to be installed
# present_bbfit(fit) will give credible intervals using the quantile method
```

11.4% more of aspirin takers don't experience a heart attack. The 95% Credible Interval (computed as the highest posterior density interval) for this difference of proportions ($p_{\text{aspirin, no attack}} - p_{\text{placebo, no attack}}$) is (0.48%, 1.09%). Aspirin takers were 1.005-1.01 times more likely to have no heart attacks. The odds of not developing a heart attack in Aspirin takers were 1.42-2.303 times the odds of those in the placebo group.

## Updating

```{r bayesian_initial, fig.width = 7, fig.height = 4}
set.seed(0)
group_1 <- sample.int(2, 20, prob = c(0.55, 0.45), replace = TRUE)-1
group_2 <- sample.int(2, 20, prob = c(0.65, 0.35), replace = TRUE)-1
fit <- beta_binom(c(sum(group_1), sum(group_2)), c(length(group_1), length(group_2)))
temp <- summary(fit); temp$term <- rownames(temp); rownames(temp) <- NULL; temp$day <- 1
posterior_summaries <- temp
```

```{r bayesian_updating, fig.width = 7, fig.height = 4}
for (day in 2:14) {
  group_1 <- sample.int(2, 10, prob = c(0.55, 0.45), replace = TRUE)-1
  group_2 <- sample.int(2, 10, prob = c(0.65, 0.35), replace = TRUE)-1
  fit <- update(fit, c(sum(group_1), sum(group_2)), c(length(group_1), length(group_2)))
  temp <- summary(fit); temp$term <- rownames(temp); rownames(temp) <- NULL; temp$day <- day
  posterior_summaries <- rbind(posterior_summaries, temp)
}
posterior_summaries$term <- factor(posterior_summaries$term,
                                   levels = c("p1", "p2", "prop_diff",
                                              "relative_risk", "odds_ratio"),
                                   labels = c("Prop 1", "Prop 2", "Prop 1 - Prop 2",
                                              "Relative Risk", "Odds Ratio"))
```

```{r visualize_updates, fig.width = 7, fig.height = 14, echo = FALSE}
posterior_summaries %>%
  { .[(.$day %% 2) == 0 | .$day == 1, ] } %>%
  ggplot(data = ., aes(x = estimate, y = term)) +
  facet_wrap(~day, nrow = 5,
             labeller = function(days) { days$day <- paste("Day", days$day); days }) +
  geom_segment(aes(x = conf.low, xend = conf.high,
                   y = term, yend = term),
               color = "#e41a1c", size = 0.75) +
  geom_point(size = 2) +
  scale_y_discrete(limits = rev(levels(posterior_summaries$term))) +
  labs(title = "Updating posterior with 10 observations/day for 2 weeks") +
  geom_segment(aes(x = 0.45, xend = 0.45, y = 4.75, yend = 5.25),
               color = "#377eb8", size = 1.1) +
  geom_segment(aes(x = 0.35, xend = 0.35, y = 3.75, yend = 4.25),
               color = "#377eb8", size = 1.1) +
  geom_segment(aes(x = 0.10, xend = 0.10, y = 2.75, yend = 3.25),
               color = "#377eb8", size = 1.1) +
  geom_segment(aes(x = 0.45/0.35, xend = 0.45/0.35, y = 1.75, yend = 2.25),
               color = "#377eb8", size = 1.1) +
  geom_segment(aes(x = (0.45/0.55)/(0.35/0.65), xend = (0.45/0.55)/(0.35/0.65),
                   y = 0.75, yend = 1.25), color = "#377eb8", size = 1.1) +
  theme(panel.grid = element_line(color = "black")) +
  scale_x_continuous(limits = c(0, 3), oob = scales::squish)
```

```{r visualize_updates_animated, fig.show = 'animate', eval = FALSE, echo = FALSE}
gg <- posterior_summaries %>%
  ggplot(data = ., aes(x = estimate, y = term, frame = day)) +
  geom_segment(aes(x = conf.low, xend = conf.high,
                   y = term, yend = term),
               color = "#e41a1c", size = 0.75) +
  geom_point(size = 2) +
  scale_y_discrete(limits = rev(levels(posterior_summaries$term))) +
  labs(title = "Estimates and HPD Intervals after day", y = NULL, x = NULL) +
  geom_segment(aes(x = 0.45, xend = 0.45, y = 4.75, yend = 5.25),
               lty = "dotted", color = "#377eb8") +
  geom_segment(aes(x = 0.35, xend = 0.35, y = 3.75, yend = 4.25),
               lty = "dotted", color = "#377eb8") +
  geom_segment(aes(x = 0.10, xend = 0.10, y = 2.75, yend = 3.25),
               lty = "dotted", color = "#377eb8") +
  geom_segment(aes(x = 0.45/0.35, xend = 0.45/0.35, y = 1.75, yend = 2.25),
               lty = "dotted", color = "#377eb8") +
  geom_segment(aes(x = (0.45/0.55)/(0.35/0.65), xend = (0.45/0.55)/(0.35/0.65),
                   y = 0.75, yend = 1.25), lty = "dotted", color = "#377eb8") +
  theme(panel.grid = element_line(color = "black"))
gganimate::gg_animate(gg, "~/Desktop/updating.gif", interval = 0.5, ani.width = 600, ani.height = 400)
```

## References

Physician's Health Study. (1988). *New England Journal of Medicine*, **318**, 262-264.

Agresti, A. (2013). Categorical Data Analysis. John Wiley & Sons.

Agresti, A., & Hitchcock, D. B. (2005). Bayesian inference for categorical data analysis. Statistical Methods and Applications.

Fienberg, S. E., & Holland, P. W. (1973). Simultaneous estimation of multinomial cell probabilities. Journal of the American Statistical Association.

Kass, R. E., & Raftery, A. E. (1995). Bayes factors. Journal of the American Statistical Association.
